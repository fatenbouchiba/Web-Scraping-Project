{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "61e2aV9gaPdk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import unicodedata\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_url(position, location):\n",
        "    base_url = \"https://remoteok.io/remote\"\n",
        "    position = position.replace(\" \", \"-\")\n",
        "    location = location.replace(\" \", \"-\")\n",
        "    return f\"{base_url}-{position}-jobs\"\n"
      ],
      "metadata": {
        "id": "2yAd7Y03abZd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Function to extract job data from a job card\n",
        "def extract_job_data(job_card):\n",
        "    # Extract job title\n",
        "    try:\n",
        "        job_title = clean_text(job_card.find(\"a\", itemprop=\"url\").find(\"h2\", itemprop=\"title\").text)\n",
        "    except AttributeError:\n",
        "        job_title = \"N/A\"\n",
        "\n",
        "    # Extract company name\n",
        "    try:\n",
        "        company_name = clean_text(job_card.find(\"span\", itemprop=\"hiringOrganization\").find(\"h3\", itemprop=\"name\").text)\n",
        "    except AttributeError:\n",
        "        company_name = \"N/A\"\n",
        "\n",
        "    # Extract location\n",
        "    try:\n",
        "        location = clean_text(job_card.find(\"div\", class_=\"location\").text)\n",
        "    except AttributeError:\n",
        "        location = \"Remote\"\n",
        "\n",
        "    # Extract date posted\n",
        "    try:\n",
        "        date_posted = job_card.find(\"time\")[\"datetime\"]\n",
        "    except (AttributeError, KeyError):\n",
        "        date_posted = \"N/A\"\n",
        "\n",
        "    # Extract job URL\n",
        "    try:\n",
        "        job_url = \"https://remoteok.io\" + job_card.find(\"a\", class_=\"preventLink\")[\"href\"]\n",
        "    except (AttributeError, KeyError):\n",
        "        job_url = \"N/A\"\n",
        "\n",
        "    return {\n",
        "        \"Job Title\": job_title,\n",
        "        \"Company Name\": company_name,\n",
        "        \"Location\": location,\n",
        "        \"Date Posted\": date_posted,\n",
        "        \"Job URL\": job_url\n",
        "    }"
      ],
      "metadata": {
        "id": "pyKj9eugbIkN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to scrape jobs\n",
        "def main(position, location):\n",
        "    url = generate_url(position, location)\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    # Send HTTP request to the URL\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve the page, status code: {response.status_code}\")\n",
        "        return\n",
        "\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Find all job cards on the page\n",
        "    job_cards = soup.find_all(\"tr\", class_=\"job\")\n",
        "\n",
        "    # List to store job data\n",
        "    job_data = []\n",
        "\n",
        "    # Loop through each job card and extract data\n",
        "    for job_card in job_cards:\n",
        "        job_info = extract_job_data(job_card)\n",
        "        job_data.append(job_info)\n",
        "\n",
        "    # Write the extracted job data to a CSV file\n",
        "    filename = f\"remoteok_{position}_{location}_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv\"\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"Job Title\", \"Company Name\", \"Location\", \"Date Posted\", \"Job URL\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(job_data)\n",
        "\n",
        "    print(f\"Scraping complete. Data saved to {filename}\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main(\"developer\", \"remote\")  # Example: Scrape for remote developer jobs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJKG3AvFdNmC",
        "outputId": "dd3f94d3-8665-42c8-fe0d-120907e81666"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Data saved to remoteok_developer_remote_20250125183921.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "main('developer', 'remote')  # Test for remote developers\n",
        "main('designer', 'remote')   # Test for remote designers\n",
        "main('data scientist', 'new york')  # Test for data scientists in New York\n",
        "main('backend engineer', 'california')  # Test for backend engineers in California\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wUElGARdkJh",
        "outputId": "8879f1ee-ce41-4aa8-d852-2f8fb4474898"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Data saved to remoteok_developer_remote_20250125184411.csv\n",
            "Scraping complete. Data saved to remoteok_designer_remote_20250125184412.csv\n",
            "Scraping complete. Data saved to remoteok_data scientist_new york_20250125184413.csv\n",
            "Scraping complete. Data saved to remoteok_backend engineer_california_20250125184413.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the CSV file generated by the scraping process\n",
        "def load_scraped_data(file_path):\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "        print(f\"Data loaded successfully from {file_path}\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {file_path} not found.\")\n",
        "        return None\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"Error: No data found in the CSV file.\")\n",
        "        return None\n",
        "\n",
        "# Check for missing values in the dataset\n",
        "def check_missing_values(data):\n",
        "    missing_values = data.isnull().sum()\n",
        "    print(\"\\nMissing Values:\")\n",
        "    print(missing_values)\n",
        "    return missing_values\n",
        "\n",
        "# Check for duplicates in the dataset\n",
        "def check_duplicates(data):\n",
        "    duplicate_rows = data.duplicated().sum()\n",
        "    print(\"\\nDuplicate Rows:\")\n",
        "    print(f\"Total duplicates found: {duplicate_rows}\")\n",
        "    return duplicate_rows\n",
        "\n",
        "# Evaluate the overall structure of the data (columns, rows, types)\n",
        "def evaluate_data_structure(data):\n",
        "    print(\"\\nData Structure Evaluation:\")\n",
        "    print(f\"Number of rows: {len(data)}\")\n",
        "    print(f\"Number of columns: {len(data.columns)}\")\n",
        "    print(f\"Columns: {data.columns.tolist()}\")\n",
        "    print(\"\\nData Types:\")\n",
        "    print(data.dtypes)\n",
        "\n",
        "# Check for any outliers or unusual values in key columns (e.g., dates or salary ranges)\n",
        "def evaluate_outliers(data):\n",
        "    # Check if the 'Date Posted' column has any unrealistic dates\n",
        "    if 'Date Posted' in data.columns:\n",
        "        data['Date Posted'] = pd.to_datetime(data['Date Posted'], errors='coerce')\n",
        "        out_of_range_dates = data[data['Date Posted'].isnull()]\n",
        "        print(\"\\nOut of Range Dates:\")\n",
        "        print(out_of_range_dates[['Job Title', 'Date Posted']])\n",
        "\n",
        "    # Check if salary ranges are available and reasonable\n",
        "    if 'Salary Range' in data.columns:\n",
        "        salary_data = data['Salary Range'].dropna()\n",
        "        # Example: Check if salary is within a reasonable range (e.g., 0 to 1M USD)\n",
        "        salary_data = salary_data.str.replace(r'[^0-9\\-]', '', regex=True)\n",
        "        salary_data = pd.to_numeric(salary_data, errors='coerce')\n",
        "        out_of_range_salaries = salary_data[(salary_data < 1000) | (salary_data > 1000000)]\n",
        "        print(\"\\nOut of Range Salaries:\")\n",
        "        print(out_of_range_salaries)\n",
        "\n",
        "# Evaluate job titles and company names for consistency\n",
        "def evaluate_job_titles_and_companies(data):\n",
        "    print(\"\\nJob Titles and Company Evaluation:\")\n",
        "    job_titles = data['Job Title'].value_counts().head(10)\n",
        "    print(\"Top 10 Most Common Job Titles:\")\n",
        "    print(job_titles)\n",
        "\n",
        "    companies = data['Company Name'].value_counts().head(10)\n",
        "    print(\"\\nTop 10 Most Common Companies:\")\n",
        "    print(companies)\n",
        "\n",
        "# Main evaluation function\n",
        "def evaluate_scraped_data(file_path):\n",
        "    data = load_scraped_data(file_path)\n",
        "    if data is not None:\n",
        "        check_missing_values(data)\n",
        "        check_duplicates(data)\n",
        "        evaluate_data_structure(data)\n",
        "        evaluate_outliers(data)\n",
        "        evaluate_job_titles_and_companies(data)\n",
        "\n",
        "# Example: Path to the CSV file generated after scraping\n",
        "file_path = \"/content/remoteok_developer_remote_20250125184411.csv\"\n",
        "evaluate_scraped_data(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1uqkONMzFki",
        "outputId": "278e4d20-5f4f-45ae-cb0a-8691313460ac"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully from /content/remoteok_developer_remote_20250125184411.csv\n",
            "\n",
            "Missing Values:\n",
            "Job Title       0\n",
            "Company Name    0\n",
            "Location        0\n",
            "Date Posted     0\n",
            "Job URL         0\n",
            "dtype: int64\n",
            "\n",
            "Duplicate Rows:\n",
            "Total duplicates found: 0\n",
            "\n",
            "Data Structure Evaluation:\n",
            "Number of rows: 19\n",
            "Number of columns: 5\n",
            "Columns: ['Job Title', 'Company Name', 'Location', 'Date Posted', 'Job URL']\n",
            "\n",
            "Data Types:\n",
            "Job Title       object\n",
            "Company Name    object\n",
            "Location        object\n",
            "Date Posted     object\n",
            "Job URL         object\n",
            "dtype: object\n",
            "\n",
            "Out of Range Dates:\n",
            "Empty DataFrame\n",
            "Columns: [Job Title, Date Posted]\n",
            "Index: []\n",
            "\n",
            "Job Titles and Company Evaluation:\n",
            "Top 10 Most Common Job Titles:\n",
            "Job Title\n",
            "Backend Engineer                  2\n",
            "Senior Backend Golang Engineer    1\n",
            "React Frontend developer          1\n",
            "Software Engineer Backend         1\n",
            "Wordpress Developer               1\n",
            "Front End lead developer          1\n",
            "Intermediate Angular Developer    1\n",
            "Software Engineer                 1\n",
            "Full Stack React Engineer         1\n",
            "Salesforce Architect              1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Top 10 Most Common Companies:\n",
            "Company Name\n",
            "Amondo                                   2\n",
            "Hippo Technologies                       1\n",
            "Dock                                     1\n",
            "BrowserStack Software Private Limited    1\n",
            "BBE Marketing                            1\n",
            "Flourish Software                        1\n",
            "emmacare.com                             1\n",
            "Revelator                                1\n",
            "Jump                                     1\n",
            "Espresso labs                            1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "WBiVpmG8dl8f"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}